{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_cigar10_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3pR/VhjGowQu6iC6vyUmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshadloo/Resnet_cifar10_pytorch/blob/main/resnet_cigar10_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wKn5vxD3EEk"
      },
      "source": [
        " ## Residual Networks\n",
        "In theory, very deep networks can represent very complex functions; but in practice, because of vanishing/exploding gradient they are hard to train. Residual Networks, introduced by [He et al.](https://arxiv.org/pdf/1512.03385.pdf), allow you to train much deeper networks than were previously practically feasible.\n",
        "\n",
        "In this notebook, I implemeneted the basic building blocks of ResNets and plain/residual network for CIFAR10 as described in the [original paper](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "In plain/residual network for CIFAR10, the network inputs are $ 32 \\times 32 \\times 3 $ images. The first layer is $3 \\times 3 $ convolutions. Then, there are 3 stages. Each stage is a stack of $2n$ layers, where $n$ is the number of basic blocks of each stage. Each basic block is a stack of 2 layers of $3 \\times 3$ convolutions. The output of 3 stages are feature maps of sizes $\\{ 32, 16, 8\\}$ respectively, and the number of filters in each layer of 3 stages are $\\{ 16, 32, 64\\}$ respectively. In residual network, there is shortcut path from input of the basic block to its output, while in plain network, there is only main path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9p6myth8C83"
      },
      "source": [
        "In the cell below, you can see implementaion of basic building block of ResNets in the class \"BasicBlock\", and  implementaion of ResNets for CIFAR10 in the class \"ResNet\". The parameter $n$, the number of basic blocks of each stage of ResNets, determines the number of layers of the model. At the end of the following cell, I provided ResNet20, ResNet32, ResNet44, ResNet56 and ResNet 110 by setting this parameter. To have corresponding plain networks, you only need to call these methods by setting the argument \"plain\" to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW04KRx50hvw"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(Lambda, self).__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, projection = False, plain = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        self.plain = plain\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            if projection:\n",
        "                # projection shortcut, as option B in paper\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels))\n",
        "\n",
        "            else :\n",
        "                # identity shortcut, as option A in paper\n",
        "                self.shortcut = Lambda(lambda x:F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, out_channels // 4, out_channels // 4),\"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if not self.plain:\n",
        "            out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    \n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_blocks = 5, num_classes=10, plain = False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        # First stack of residual blocks, output is 16 x 32 x 32\n",
        "        self.layer1 = self.residual_layer(16, num_blocks, stride=1, plain = plain)\n",
        "        # Second stack of residual blocks, output is 32 x 16 x 16\n",
        "        self.layer2 = self.residual_layer(32, num_blocks, stride=2,plain = plain)\n",
        "        # Third stack of residual blocks, output is 64 x 8 x 8\n",
        "        self.layer3 = self.residual_layer( 64, num_blocks, stride=2,plain = plain)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        # import pdb; pdb.set_trace()\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "    def residual_layer(self, out_channels, num_blocks, stride, plain = False):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride, plain=plain))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks-1):\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels, stride=1, plain=plain))\n",
        "            self.in_channels = out_channels\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "def resnet20(plain = False):\n",
        "    return ResNet(3, plain= plain)\n",
        "\n",
        "\n",
        "def resnet32(plain = False):\n",
        "    return ResNet(5,plain= plain)\n",
        "\n",
        "\n",
        "def resnet44(plain = False):\n",
        "    return ResNet(7, plain= plain)\n",
        "\n",
        "\n",
        "def resnet56(plain = False):\n",
        "    return ResNet(9, plain= plain)\n",
        "\n",
        "\n",
        "def resnet110(plain = False):\n",
        "    return ResNet(18, plain= plain)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBSZ4uprH87s"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZABUQzcDlj6"
      },
      "source": [
        "### Google Colab\n",
        "To train provided models, you required GPU. You can use GPUs of Google Colab. If you are using Google Colab and if the runtime restarts during training, you will lose your trained model. Then you have to start again from the scratch, which is not optimal. Instead, you should save your model checkpoint to Google Drive and reload it next time when you start. To do so, you need to mount your Google Drive  and give permission to Google Colab to access it. Also you need to create a directory in your Google Drive for this project. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaOSG7EqCfxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc99ed8-ea75-478d-f50d-ddf773601985"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiaD6LXsNJ8D"
      },
      "source": [
        "working_dir = \"./drive/MyDrive/res_cifar10_pytorch\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVd4HpT3LK3V"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8VDyCk3COoX"
      },
      "source": [
        "import easydict\n",
        "args = easydict.EasyDict({\n",
        "    \"res_model\":\"resnet32\",\n",
        "    \"epochs\": 200,\n",
        "    \"batch_size\": 128, \"lr\":0.1, 'momentum':0.9, 'weight_decay':5e-4,'save_dir':'save_dir','data_dir':'data','plain_mode':False,'resume':''\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezeo5Ne2LFAp"
      },
      "source": [
        "Let's run the cell below to load the required packages and set the directories for saving data, models and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI4_CSSAKank"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "\n",
        "data_dir = os.path.join(working_dir, args.data_dir)\n",
        "save_dir = os.path.join(working_dir, args.save_dir)\n",
        "model_name = args.res_model  +(\"_plain5\" if args.plain_mode else \"\")\n",
        "checkpoint_name = \"_\".join([ model_name,'checkpoint.th'])\n",
        "models={\"resnet20\":resnet20, \"resnet32\":resnet32, \"resnet44\":resnet44,\"resnet56\":resnet56,\"resnet110\":resnet110 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI9MM20hIzpP"
      },
      "source": [
        "### Dataset\n",
        "I load and normalize CIFAR10 training and test datasets using torchvision. The training dataset contains 50K images and the test dataset contains 10K images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeizSdfzJN9z"
      },
      "source": [
        "def load_data():\n",
        "    print('==> Loading data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform_train)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size= args.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader,test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FA3yK3eNzyu"
      },
      "source": [
        "### One step of training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqmyp2vfN47h"
      },
      "source": [
        "def make_train_step(train_loader, model, loss_fn, optimizer):\n",
        "    def train_step(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        data_count = 0\n",
        "        \n",
        "        for batch_ix, (inputs, targets) in enumerate(train_loader):\n",
        "            curr_time = time.time()\n",
        "      \n",
        "            if torch.cuda.is_available():\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "           \n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          \n",
        "            outputs = outputs.float()\n",
        "\n",
        "            _, predictions = outputs.max(1)\n",
        "\n",
        "            batch_size = inputs.size(0)\n",
        "            loss = loss.float()\n",
        "            train_loss += loss.item() * batch_size\n",
        "            data_count += batch_size\n",
        "            correct += predictions.eq(targets).sum().item()\n",
        "\n",
        "            if batch_ix % 50 == 0:\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'.format(\n",
        "                    epoch, batch_ix, len(train_loader)))\n",
        "                print(\"It took {:.3f}s\".format(\n",
        "                    time.time() - curr_time))\n",
        "                print(\"  training loss:\\t\\t{:.6f}\".format(train_loss / (data_count)))\n",
        "\n",
        "        return train_loss / data_count, 100. * correct / data_count\n",
        "    return train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK2dIqzQOEwR"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH8RHoQcLPTl"
      },
      "source": [
        "def evaluate(test_loader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    data_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            outputs = model(inputs).float()\n",
        "            loss = loss_fn(outputs, targets).float()\n",
        "\n",
        "            test_loss += loss.item()*targets.size(0)\n",
        "            _, predictions = outputs.max(1)\n",
        "            data_count += targets.size(0)\n",
        "            correct += predictions.eq(targets).sum().item()\n",
        "    return test_loss / data_count, 100. * correct / data_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ewdi3QnOIaQ"
      },
      "source": [
        "### Saving results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s4gc02TOQem"
      },
      "source": [
        "def save_results(test_acc, train_acc, best_acc):\n",
        "    f = {}\n",
        "    f['test_acc'] = test_acc\n",
        "    f['train_acc'] = train_acc\n",
        "    f['best_acc'] = best_acc\n",
        "\n",
        "    filename = os.path.join(save_dir, model_name)\n",
        "    fName = open(filename + \".pkl\", \"wb\")\n",
        "    pickle.dump(f, fName)\n",
        "    fName.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYSzSEz1Orja"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H7Yn76GOxBm"
      },
      "source": [
        "  best_acc =0\n",
        "\n",
        "  train_loader, test_loader = load_data()\n",
        "  print(\"==>Building model...\")\n",
        "  # model = resnet.__dict__[args.res_model](plain = args.plain_mode)\n",
        "  model = models[args.res_model](plain = args.plain_mode)\n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "      model = torch.nn.DataParallel(model)\n",
        "      cudnn.benchmark = True\n",
        "  if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  if torch.cuda.is_available():\n",
        "      loss_fn = loss_fn.cuda()\n",
        "\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                              momentum=args.momentum,\n",
        "                              weight_decay=args.weight_decay)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60,100,150,180],gamma=0.1, last_epoch=-1)\n",
        "  # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "  # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1, last_epoch=-1) \n",
        "\n",
        "\n",
        "  loss_train, acc_train, loss_test, acc_test = [], [], [], []\n",
        "  train_step = make_train_step(train_loader, model, loss_fn, optimizer)\n",
        "  print(\"==>Starting training...\")\n",
        "  for epoch in range(args.epochs):\n",
        "\n",
        "      curr_time = time.time()\n",
        "      loss, acc = train_step(epoch)\n",
        "      train_time = time.time() - curr_time\n",
        "\n",
        "\n",
        "      loss_train.append(loss)\n",
        "      acc_train.append(acc)\n",
        "      lr_scheduler.step()\n",
        "\n",
        "      loss, acc = evaluate(test_loader, model, loss_fn)\n",
        "\n",
        "\n",
        "      loss_test.append(loss)\n",
        "      acc_test.append(acc)\n",
        "\n",
        "      if acc > best_acc:\n",
        "          print('Saving..')\n",
        "          state = {\n",
        "              'state_dict': model.state_dict(),\n",
        "              'acc': acc,\n",
        "              'epoch': epoch,\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "          }\n",
        "          filename = os.path.join(save_dir, checkpoint_name)\n",
        "          torch.save(state, filename)\n",
        "          best_acc = acc\n",
        "      print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "          epoch + 1, args.epochs, train_time))\n",
        "      print(\"  training loss:\\t\\t{:.6f}\".format(loss_train[-1]))\n",
        "      print(\"  test loss:\\t\\t{:.6f}\".format(loss_test[-1]))\n",
        "      print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "          acc_test[-1]))\n",
        "  save_results(acc_test, acc_train, best_acc)\n",
        "  print(\"best error:\",1 - (best_acc / 100. ) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}